---
layout: post
title: "¿Es la falta de generalización un problema de la IA?"
date: 2025-11-26
---

26 de noviembre de 2025

Dwarkesh Patel entrevistó a Ilya Sutskever, cofundador de Safe Superintelligence Inc (SSI). Sutskever dijo que estamos pasando de la "era del escalado" (2020-2025) de vuelta a la "era de la investigación". Para él, el mayor problema de los modelos actuales es que generalizan mucho peor que los humanos: pueden ser brillantes en evaluaciones específicas como competencias de programación, pero fallan en tareas básicas que necesitan criterio y adaptabilidad. Esto pasa porque el entrenamiento por refuerzo se centra demasiado en optimizar para tests concretos en lugar de desarrollar una comprensión profunda. Patel conversó con Sutskever sobre estas ideas, y este último planteó que hay que repensar desde cero cómo entrenamos estos modelos. Quizás incorporando funciones de valor más avanzadas y buscando nuevas formas de mejorar la generalización, porque simplemente multiplicar por 100 la potencia de cómputo no va a solucionar estos problemas básicos de aprendizaje y robustez.

Solo el tiempo le dará la razón o se la quitará a Ilya. Hoy resulta imposible saber si su intuición sobre los límites del escalado es correcta o si será refutada por los próximos avances. Como pasa con la mayoría de dudas en la vida, el tiempo te ofrecerá la respuesta que buscas. Aunque como esto es un blog personal y aquí comparto mis opiniones, yo pienso que su predicción no se cumplirá o se cumplirá con un porcentaje de acierto o puesta en práctica muy bajo, que la IA va a avanzar limitada a las empresas ofreciendo pequeñas mejoras cada año, como desgraciadamente ocurre en una economía del compra, consume, rompe y vuelve a comprar en la que cada año necesitas ese nuevo iPhone. Por supuesto, en mi caso también el tiempo dirá si estaba acertado o equivocado.
